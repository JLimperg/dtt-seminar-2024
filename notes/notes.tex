\documentclass{scrartcl}
\usepackage{fontspec}
\usepackage{mathtools}
\usepackage{unicode-math}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{csquotes}
\usepackage{mathpartir}
\usepackage{datetime}
\usepackage[colorlinks,linkcolor=blue]{hyperref}

\setmainfont{XITS}
\setmathfont{XITS Math}
\setmonofont[Scale=MatchLowercase]{DejaVu Sans Mono}

\addtokomafont{disposition}{\rmfamily} % serif headings

\theoremstyle{definition}
\newmdtheoremenv[innertopmargin=0pt]{definition}{Definition}
\newmdtheoremenv[innertopmargin=0pt]{example}[definition]{Example}
\newmdtheoremenv[innertopmargin=0pt]{lemma}[definition]{Lemma}
\newmdtheoremenv[innertopmargin=0pt]{theorem}[definition]{Theorem}
\newmdtheoremenv[innertopmargin=0pt]{corollary}[definition]{Corollary}

\bibliographystyle{plain}

\newcommand{\Vars}{\mathcal{V}}
\newcommand{\Base}{\mathcal{B}}
\newcommand{\Types}{\mathcal{T}}
\newcommand{\Terms}{\mathcal{S}}
\newcommand{\Ctxs}{\mathcal{C}}
\newcommand{\bv}{\mathrm{bv}}
\newcommand{\fv}{\mathrm{fv}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\cod}{\mathrm{cod}}
\newcommand{\fresh}{\mathrm{fresh}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\const}{\mathrm{const}}
\newcommand{\app}{\mathrm{app}}
\newcommand{\Lam}[2]{λ\,#1.\, #2}
\newcommand{\All}[2]{∀\,#1.\, #2}
\newcommand{\Ex}[2]{∃\,#1.\, #2}
\newcommand{\PiT}[2]{Π\,#1.\, #2}
\newcommand{\SigT}[2]{Σ\,#1.\, #2}
\newcommand{\unit}{\mathrm{unit}}
\newcommand{\absurd}{\mathrm{absurd}}
\newcommand{\pair}{\mathrm{pair}}
\newcommand{\zero}{\mathrm{zero}}
\newcommand{\suc}{\mathrm{succ}}
\newcommand{\elimSig}{\mathrm{elimΣ}}
\newcommand{\elimTop}{\mathrm{elim⊤}}
\newcommand{\elimBot}{\mathrm{elim⊥}}
\newcommand{\elimSum}{\mathrm{elim+}}
\newcommand{\elimId}{\mathrm{elim{=}}}
\newcommand{\elimNat}{\mathrm{elimℕ}}
\newcommand{\El}{\mathrm{El}}
\newcommand{\ctx}{\mathrm{ctx}}
\newcommand{\type}{\mathrm{type}}
\newcommand{\Univ}{\mathcal{U}}
\newcommand{\projl}{\ensuremath{\mathrm{projl}}}
\newcommand{\projr}{\ensuremath{\mathrm{projr}}}
\newcommand{\inl}{\ensuremath{\mathrm{inl}}}
\newcommand{\inr}{\ensuremath{\mathrm{inr}}}
\renewcommand{\Vec}{\ensuremath{\mathrm{Vec}}}
\newcommand{\List}{\ensuremath{\mathrm{List}}}
\newcommand{\head}{\ensuremath{\mathrm{head}}}
\newcommand{\tail}{\ensuremath{\mathrm{tail}}}
\newcommand{\length}{\ensuremath{\mathrm{length}}}
\newcommand{\case}{\mathrm{case}}
\newcommand{\refl}{\mathrm{refl}}
\newcommand{\lbl}[1]{\RightTirNameStyle{#1}}
\newcommand{\infr}[3]{\inferrule*[right=#1]{#2}{#3}}

\newenvironment{tinymathpar}{%
  \begin{mathpar}
  \tiny
  \renewcommand{\RightTirNameStyle}[1]{\tiny{\textsc{##1}}}
}{%
  \end{mathpar}
}

\begin{document}

\title{A Brief Introduction to Dependent Type Theory}
\author{Jannis Limperg}
\date{Last updated \today{} at \currenttime}
\maketitle

\section{Introduction}

With these lecture notes, I aim to give readers the necessary background for (introductory) papers in dependent type theory.
A major obstacle in this endeavour is that the term \enquote{type theory} refers not to one or two well-specified and well-studied formal systems but to a wide variety of systems with very different purposes and properties.
Additionally, the presentation of these systems differs according to the aesthetic preferences of their authors.
I try to represent a reasonable portion of this variety in these notes, but as a result the discussion remains fairly shallow.
Also, readers should consider the systems which are discussed primarily as examples, focusing on the techniques for defining and analysing these systems rather than on the systems themselves.

\section{Inductively Defined Relations}%
\label{sec:indrel}

Type theories are typically defined as collections of inductively defined sets and relations.
We therefore first review this central technique, starting with a typical example.

\begin{definition}[Transitive closure]
  Let $A$ be a set and $R ⊆ A × A$ a binary relation on $A$.
  The \emph{transitive closure} of $R$, written $R^{+}$, is inductively defined by the following rules:
  \begin{itemize}
    \item For all $a,b ∈ A$, if $aRb$ then $aR^{+}b$.
    \item For all $a,b,c ∈ A$, if $aR^{+}b$ and $bR^{+}c$ then $aR^{+}c$.
  \end{itemize}
\end{definition}

\enquote{Inductively defined} means that two elements $a,b ∈ A$ are related by $R^{+}$ if and only if we can prove this with finitely many applications of the given rules.%
\footnote{This is not quite true in general: if a rule has infinitely many premises, then to apply this rule we must provide infinitely many subproofs.
  However, we will only use rules with finitely many premises.}
Equivalently, $R^{+}$ is the smallest binary relation on $A$ that is closed under the given rules.

\begin{example}\label{ex:transitive-closure}
  Suppose $a,b,c,d ∈ A$ with $aRb$, $bRc$ and $cRd$.
  Then $aR^{+}d$ since we can reach this conclusion with finitely many applications of the rules of $R^{+}$:
  \begin{itemize}
    \item From $aRb$ follows $aR^{+}b$.
    \item From $bRc$ follows $bR^{+}c$.
    \item From $aR^{+}b$ and $bR^{+}c$ follows $aR^{+}c$.
    \item From $cRd$ follows $cR^{+}d$.
    \item From $aR^{+}c$ and $cR^{+}d$ follows $aR^{+}d$.
  \end{itemize}
\end{example}

We usually write the rules of an inductive relation as inference rules, with the premises of the rule above the horizontal bar and the conclusion below.
Any variables occurring in an inference rule are implicitly universally quantified.
The rules of $R^{+}$ would therefore usually be written as follows.

\begin{mathpar}
  \infr{Step}{aRb}{aR^{+}b}

  \infr{Trans}{aR^{+}b \\ bR^{+}c}{aR^{+}c}
\end{mathpar}

The labels \lbl{Step} and \lbl{Trans} serve as names for the rules.
We may now recast our proof of $aR^{+}d$ from example~\ref{ex:transitive-closure} as a tree of instantiations of the inference rules of $R^{+}$.
Such trees are called \emph{derivation trees} or \emph{proof trees}.

\begin{mathpar}
  \infr{Trans}
    {\infr{Trans}
      {\infr{Step}
        {\infr{}{ }{aRb}}
        {aR^+b}
       \\
       \infr{Step}
        {\infr{}{ }{bRc}}
        {bR^+c}}
      {aR^+c}
     \\
     \infr{Step}
      {\infr{}{ }{cRd}}
      {cR^{+}d}}
    {aR^{+}d}
\end{mathpar}

We now define some more useful constructions on binary relations $R$ over a set $A$.

\begin{definition}[Reflexive closure]
  The \emph{reflexive closure} of $R$, $S$, is inductively defined by the following rules:

  \begin{mathpar}
    \infr{Step}{aRb}{aSb}

    \infr{Refl}{  }{aSa}
  \end{mathpar}
\end{definition}

Equivalently, $S = R ∪ \{(a, a) \mid a ∈ A\}$.

\begin{definition}[Reflexive-transitive closure]\label{def:refl-trans-clos}
  The \emph{reflexive-transitive closure} of $R$, written $R^{*}$, is inductively defined by the following rules:

  \begin{mathpar}
    \infr{Step}{aRb}{aR^{*}b}

    \infr{Trans}{aR^{*}b \\ bR^{*}c}{aR^{*}c}

    \infr{Refl}{  }{aR^{*}a}
  \end{mathpar}
\end{definition}

Equivalently, $R^{*}$ is the reflexive closure of $R^{+}$.

\begin{definition}[Symmetric closure]
  The \emph{symmetric closure} of $R$, written $R^{↔}$, is inductively defined by the following rules:

  \begin{mathpar}
    \infr{Step}{aRb}{aR^{↔}b}

    \infr{Sym}{aRb}{bR^{↔}a}
  \end{mathpar}
\end{definition}

Equivalently, $R^{↔} = R ∪ \{(b, a) \mid (a, b) ∈ R\}$.

\begin{definition}[Equivalence closure]\label{def:equivalence-closure}
  The \emph{reflexive-symmetric-transitive} or \emph{equivalence closure} of $R$, written $R^{=}$, is inductively defined by the following rules:

  \begin{mathpar}
    \infr{Step}{aRb}{aR^{=}b}

    \infr{Refl}{  }{aR^{=}a}

    \infr{Sym}{aRb}{bR^{=}a}

    \infr{Trans}{aR^{=}b \\ bR^{=}c}{aR^{=}c}
  \end{mathpar}
\end{definition}

Equivalently, $R^{=}$ is the symmetric closure of $R^{*}$.

\section{The Simply-Typed Lambda Calculus}

The simply-typed lambda calculus (STLC) is a minimalistic typed functional programming language that can also be viewed as a proof system for propositional logic.
Type theories can be viewed as extensions of STLC, so it will be useful to consider some concepts and terminology in the simple setting of STLC before moving to type theories.

To that end, we first define the types and terms (programs) of STLC.\@

\begin{definition}[Variables]
  We assume a countably infinite set $\Vars$ of variables.
  We identify variables with strings, e.g.\ $x$, $y$, $\mathit{foo}$.
\end{definition}

\begin{definition}[Base Types]
  We assume a nonempty set $\Base$ of \emph{base types}.
\end{definition}

\begin{definition}[Types]
  The set $\Types$ of \emph{STLC types} is inductively generated by the following rules:

  \begin{itemize}
    \item If $T$ is a base type, then $T ∈ \Types$.
    \item If $T$ and $U$ are STLC types, then $(T → U) ∈ \Types$.
  \end{itemize}
\end{definition}

The type $T → U$ is the type of functions with domain $T$ and codomain $U$.
The arrow associates to the right, so $T → U → V = T → (U → V)$.

\begin{definition}[Terms]\label{def:stlc-terms}
  The set $\Terms$ of \emph{STLC terms} is inductively generated by the following rules:

  \begin{itemize}
    \item Each $x \in \Vars$ is an STLC term.
    \item If $t$ and $u$ are STLC terms, then $t~u$ is an STLC term.
          We call $t~u$ the \emph{application} of $t$ to $u$.
    \item If $x$ is a variable, $T$ is an STLC type and $t$ is an STLC term, then $\Lam{x : T}{t}$ is an STLC term.
          We call $\Lam{x : T}{t}$ a \emph{function} or \emph{lambda abstraction} or \emph{abstraction}.
  \end{itemize}
\end{definition}

If the type $T$ of a lambda abstraction $\Lam{x : T}{t}$ is clear from context, we write $\Lam{x}{t}$.
In regular mathematical notation, we would write $t(u)$ instead of $t~u$ and $x ↦ t$ for $\Lam{x}{t}$.
Application associates to the left, so $t~u~v = (t~u)~v$.
The scope of an abstraction extends as far to the right as possible, so $\Lam{x : T}{t~u} = \Lam{x : T}{(t~u)}$ and $\Lam{x : T}{\Lam{y : U}{t~u}} = \Lam{x : T}{(\Lam{y : U}{(t~u)})}$.
For the remainder of this section, we drop the \enquote{STLC} prefix, so we refer to STLC types as just \emph{types} and to STLC terms as just \emph{terms}.

Here are two examples of STLC programs:

\begin{example}
  For any type $T$, we define the \emph{identity function at $T$} as the term $\id_{T} \coloneqq \Lam{x : T}{x}$.
  This is the function that maps each element of $T$ to itself, i.e.\ $x \mapsto x$.
\end{example}

\begin{example}
  For any two types $T$ and $U$, we define the \emph{constant function at $T$ and $U$} as the term $\const_{T,U} \coloneqq \Lam{x : T}{\Lam{y : U}{x}}$.
  This is the function that takes two arguments, ignores the second one and returns the first one, i.e.\ $(x, y) ↦ x$.
\end{example}

The previous example illustrates that even though STLC only provides functions with one argument, we can simulate functions with multiple arguments.
This is called \emph{currying} (after the mathematician Haskell Curry).
The central observation is that a function with two arguments can be read as a function with one argument which returns a function with another argument.
In mathematical notation: a function $f \coloneqq (x, y) ↦ x$ is equivalent to $g \coloneqq x ↦ (y ↦ x)$ since we can define $f(x, y) \coloneqq g(x)(y)$.
In STLC, we cannot define $f$ since STLC does not have pairs (though these can be added), but we can define $g \coloneqq \Lam{x}{\Lam{y}{x}}$.
The application of $g$ to two arguments $x$ and $y$ is written $(g~x)~y$ in STLC, and since application associates to the left, we usually write $g~x~y$.
Additionally, we abbreviate nested abstractions $\Lam{x : T}{\Lam{y : U}{t}}$ as $\Lam{(x : T)~(y : U)}{t}$ (and similar for more than two arguments).
These syntactic conventions allow us to think about functions as having multiple arguments without having to add new syntactic constructs.

\subsection{Typing}

STLC has typed functions, but nothing so far forces us to respect the types.
For example, the term $\Lam{x : T}{\Lam{y : U}{x~y}}$ is a perfectly good STLC term even if $T$ is not a function type but a base type, e.g.\ the type of booleans or natural numbers.
This is problematic because for an application to make sense, the term being applied must be a function.
We therefore define a \emph{type system} which identifies those terms which we would intuitively consider sensible.

\begin{definition}[Contexts]
  An \emph{STLC context} is a partial map from variables to types.
  The empty context $∅$ is the partial map with empty domain.
  Given a context $Γ$, a variable $x$ and a type $T$, $Γ,\, x : T$ is the partial map that maps $x$ to $T$ and any other variable $y$ to $Γ(y)$.
\end{definition}

\begin{definition}[Typing]\label{def:stlc-typing}
  The \emph{STLC typing relation} is a subset of $\Ctxs × \Terms × \Types$.
  If a context $Γ$, term $t$ and type $T$ are in this relation, we write $Γ ⊢ t : T$.
  The typing relation is defined inductively by the following rules:

  \begin{mathpar}
    \infr{Var}{Γ(x) = T}{Γ ⊢ x : T}

    \infr{App}{Γ ⊢ f : T → U \\ Γ ⊢ t : T}{Γ ⊢ t~u : U}

    \infr{Abs}{Γ,\, x : T ⊢ t : U}{Γ ⊢ \Lam{x : T}{t} : T → U}
  \end{mathpar}

  All rules have additional premises, left implicit for readability, which ensure that the rule references the right kinds of objects.
  E.g.\ the $\lbl{Var}$ rule has premises $Γ \in \Ctxs$, $x ∈ \Vars$ and $T ∈ \Types$.

  We abbreviate $∅ ⊢ t : T$ as $⊢ t : T$.
  A term $t$ is \emph{well-typed} if there is a type $T$ such that $⊢ t : T$.
  A term $t$ is \emph{well-typed in context $Γ$} if there is a type $T$ such that $Γ ⊢ t : T$.
\end{definition}

The symbols $⊢$ and $:$ in $Γ ⊢ t : T$ have lower precedence than any STLC syntax, so the conclusion of the \lbl{Abs} rule should be read as $Γ ⊢ (\Lam{x : T}{t}) : T → U$.
We are usually only interested in well-typed terms (in some ambient context).
Many authors therefore call our terms \enquote{preterms} or \enquote{pseudoterms} and reserve the word \enquote{term} for well-typed preterms.

\begin{example}
  We show that the constant function at types $T$ and $U$ is well-typed, i.e.\ $⊢ \Lam{x : T}{\Lam{y : U}{x} : T → U → T}$.
  Since the typing relation is inductively defined, a proof of $Γ ⊢ t : T$ for some $Γ$, $t$ and $T$ is a proof tree

  This is done by repeatedly applying the rules of the STLC typing relation, arranging them in a derivation tree as in Section~\ref{sec:indrel}:

  \begin{mathpar}
    \infr{Abs}
        {\infr{Abs}
            {\infr{Var}
                {\infr{}
                    { }
                    {(∅,\, x : T,\, y : U)(x) = T}}
                {∅,\, x : T,\, y : U ⊢ x : T}}
            {∅,\, x : T ⊢ \Lam{y : U}{x} : U → T}}
        {⊢ \Lam{x : T}{\Lam{y : U}{x} : T → U → T}}
  \end{mathpar}
\end{example}

\begin{example}
  We show that the \emph{application function at types $T$ and $U$}, $\app_{T,U} \coloneqq \Lam{f : T → U}{\Lam{t : T}}{f~t}$, is well-typed.

  \begin{tinymathpar}
    \infr{Abs}
        {\infr{Abs}
            {\infr{App}
                {\infr{Var}
                    {\infr{}{ }{(∅,\, f : T → U,\, t : T)(f) = T → U}}
                    {∅,\, f : T → U,\, t : T ⊢ f : T → U}
                 \\
                 \infr{Var}
                    {\infr{}{ }{(∅,\, f : T → U,\, t : T)(t) = T}}
                    {∅,\, f : T → U,\, t : T ⊢ t : T}
                }
                {∅,\, f : T → U,\, t : T ⊢ f~t : U}}
            {∅,\, f : T → U ⊢ \Lam{t : T}{f~t} : T → U}}
        {⊢ \Lam{f : T → U}{\Lam{t : T}}{f~t} : (T → U) → T → U}
  \end{tinymathpar}
\end{example}

\subsection{Bound and Free Variables}

An abstraction $\Lam{x : T}{t}$ binds the variable $x$, so $x$ is in scope in the abstraction's body $t$ (but not outside it).
In this sense, abstractions are similar to the quantifiers $\All{x}{P(x)}$ and $\Ex{x}{P(x)}$ of predicate logic, and so we define similar notions of bound and free variables.

\begin{definition}[Bound variables]
  The \emph{bound variables} of a term $t$, $\bv(t)$, are those variables which are bound by an abstraction.
  We define $\bv(t)$ by recursion on $t$:

  \begin{align*}
    \bv(x) &\coloneqq ∅ \\
    \bv(t~u) &\coloneqq \bv(t) ∪ \bv(u) \\
    \bv(\Lam{x : T}{u}) &\coloneqq \bv(u) ∪ \{x\}
  \end{align*}
\end{definition}

\begin{definition}[Free variables]
  The \emph{free variables} of a term $t$, $\fv(t)$, are those variables which appear in $t$ without being bound by an abstraction.
  We also define $\fv(t)$ by recursion on $t$:

  \begin{align*}
    \fv(x) &\coloneqq \{x\} \\
    \fv(t~u) &\coloneqq \fv(t) ∪ \fv(u) \\
    \fv(\Lam{x : T}{u}) &\coloneqq \fv(u) \setminus \{x\}
  \end{align*}

  A term $t$ is \emph{closed} if $\fv(t) = ∅$.
\end{definition}

\subsection{α-Equivalence}

What is the difference between the terms $t ≔ \Lam{x : T}{x}$ and $u ≔ \Lam{y : T}{y}$?
Nothing except the syntactic detail that we chose $x$ as the variable name for $t$ and $y$ for $u$.
We would therefore like to consider $t$ and $u$ as \enquote*{the same} function, and so we define an equivalence relation that relates precisely those terms which are equal up to renaming of bound variables.

\begin{definition}[α-equivalence]
  Two terms $t$ and $u$ are \emph{α-equivalent}, written $t ≡_{α} u$, if $t$ can be made equal to $u$ by renaming bound variables.
\end{definition}

The principle that variable names do not matter allows us to avoid \enquote*{collisions} between the bound variables of a term and variables occurring elsewhere.

\begin{lemma}\label{lem:fresh-terms}
  Given a finite set $V$ of variables and a term $t$, there is a term $t'$ such that $t ≡_{α} t'$ and $\bv(t') ∩ V = ∅$.
  We define $\fresh(t, V)$ as $t$ if $\bv(t') ∩ V = ∅$ and otherwise as an arbitrary $t'$ satisfying this condition.
\end{lemma}

\begin{proof}
  The sets $V$ and $\bv(t)$ are finite while the set of variables $\Vars$ is countably infinite.
  We can thus select $|\bv(t)|$ variables from $\Vars$ which are not in $V$ and rename the bound variables in $t$ to obtain $t'$.
\end{proof}

\subsection{Substitution}

If we want to consider STLC as a programming language, we need to explain how a program (term) is evaluated.
A key ingredient for this is substitution, which is used to define the evaluation of function applications:
the application $(\Lam{x : T}{t})~u$ evaluates to $t[x ↦ u]$, which is the term $t$ with every free occurrence of the variable $x$ replaced by $u$.
This is called the substitution of $u$ for $x$ in $t$.

\begin{definition}[Substitution]
  A \emph{(simultaneous) substitution} $σ$ is a partial map from variables to terms.
  The domain of $σ$ must be finite.
  We write $\{x₁ ↦ t₁, \dots, xₙ ↦ tₙ\}$ for the substitution $σ$ with domain $\{x₁, \dots xₙ\}$ and $σ(xᵢ) = tᵢ$ for all $i ∈ \{1, \dots, n\}$.
\end{definition}

This definition is somewhat more general than that often given in the literature, which only considers the substitution of one variable at a time.
But our definition is more natural for certain applications, especially the definition of category-theoretical models of type theory, in which substitutions play a surprisingly important part.

For the application of a substitution to a term, we first consider an almost-correct definition:

\begin{definition}[Naive application of a substitution]
  The \emph{naive application} of a substitution $σ$ to a term $t$, written $t⟨σ⟩$, is defined by recursion on $t$:

  \begin{align*}
    x⟨σ⟩ &\coloneqq σ(x) \\
    (t₁~t₂)⟨σ⟩ &≔ t₁⟨σ⟩~t₂⟨σ⟩ \\
    (\Lam{x : T}{t})⟨σ⟩ &≔ \Lam{x : T}{t⟨σ⟩}
  \end{align*}
\end{definition}

We abbreviate $t⟨\{x₁ ↦ t₁, \dots, xₙ ↦ tₙ\}⟩$ as $t⟨x₁ ↦ t₁, \dots, xₙ ↦ tₙ⟩$.
Substitution applications have higher precedence than term operators, so $t~u[σ]$ is the application of the term $t$ to the term $u[σ]$.

The first two cases of the naive definition are sensible, but the third has two subtle flaws.
First, it ignores the fact that $x$ is bound by the lambda abstraction, so we get $(\Lam{x : T}{t})⟨x ↦ u⟩ = \Lam{x : T}{u}$ even though the substitution should only replace free occurrences of $x$.
Second, substitution as defined above can also introduce new bindings: $(\Lam{x : T}{y})⟨x ↦ y⟩ = \Lam{x : T}{x}$.
We say that the substitution has \emph{captured} $x$, which was previously free; this is also incorrect.
To solve both problems, we define \emph{capture-avoiding substitution}.

\begin{definition}[Capture-avoiding application of a substitution]
  The \emph{capture-avoiding application} of a substitution $σ$ to a term $t$ is
  \[
    t[σ] ≔ \fresh(t, \dom(σ) ∪ \bigcup_{u ∈ \cod(σ)} \fv(u))⟨σ⟩
  \]
\end{definition}

This means that we first rename all bound variables in $t$ to avoid collisions with variables that are either substituted by $σ$ or that occur freely in the codomain of $σ$.
After this renaming, the naive application of $σ$ can no longer lead to collisions.

All this concern about variable names is in some sense a technical triviality since we know that we can always rename variables as necessary to avoid collisions.
Some papers therefore use the \emph{Barendregt convention}, which states that whenever there is a risk of collision, we implicitly rename variables to avoid it.
Effectively, the $\fresh$ function is applied implicitly.
This is very convenient on paper and we will use the convention henceforth.
But formal developments of type theories cannot take such liberties.

\subsection{Small-Step Operational Semantics}

Terms were so far mere syntactic constructs which we only informally interpreted as functions.
We now make this interpretation precise by giving terms a meaning, or semantics, which treats them as programs that can be evaluated.

\begin{definition}[Single-step evaluation relation]\label{def:single-step}
  The \emph{single-step evaluation relation} $↝$ is a subset of $\Terms × \Terms$.
  It is defined inductively by the following rules:

  \begin{mathpar}
    \infr{β}{ }{(\Lam{x : A}{t})~u ↝ t[x ↦ u]}

    \infr{CongAppL}{t ↝ t'}{t~u ↝ t'~u}

    \infr{CongAppR}{u ↝ u'}{t~u ↝ t~u'}
  \end{mathpar}
\end{definition}

For terms $t,u$ with $t ↝ u$, we say that $t$ \emph{reduces to} or \emph{evaluates to} $u$ in one \emph{reduction} or \emph{evaluation step}.
We also call $↝$ the single-step \emph{reduction relation}.
The relation $↝$ has one \enquote*{substantive} rule, \lbl{β}, and two congruence rules, \lbl{CongAppL} and \lbl{CongAppR}.
The \lbl{β} rule specifies how a function application is reduced.
An application of this rule is called a \emph{β-reduction}.
The congruence rules allow us to apply the $β$ rule at an arbitrary location in a term, as the next example shows.

\begin{example}\label{ex:untyped-single-step}
  We show that $\const_{T,U}~x~y ↝ (\Lam{y : U}{x})~y$.

  \begin{mathpar}
    \infr{CongAppL}
      {\infr{β}
        { }
        {(\Lam{x : T}{\Lam{y : U}{x}})~x ↝ \Lam{y : U}{x}}}
      {(\Lam{x : T}{\Lam{y : U}{x}})~x~y ↝ (\Lam{y : U}{x})~y}
  \end{mathpar}
\end{example}

So far, we have only specified how to perform one computation step (namely, one $β$-reduction).
But with this, it is easy to define how to evaluate a program in multiple computation steps.

\begin{definition}[Multi-step evaluation relation]
  The \emph{multi-step evaluation relation} $↝^{*}$ is the reflexive-transitive closure of the single-step evaluation relation $↝$.
\end{definition}

\begin{example}
  We show that $\const_{T,U}~x~y ↝^{*} x$.

  \begin{mathpar}
    \infr{Trans}
    { \infr{Step}
      {\infr{}
        {\text{\upshape (example~\ref{ex:untyped-single-step})}}
        {\const_{T,U}~x~y ↝ (\Lam{y : U}{x})~y}}
      {\const_{T,U}~x~y ↝^* (\Lam{y : U}{x})~y}
      \\
      \infr{Step}
      { \infr{β}
        { }
        {(\Lam{y : U}{x})~y ↝ x}}
      {(\Lam{y : U}{x})~y ↝^* x}}
    {\const_{T,U}~x~y ↝^{*} x}
  \end{mathpar}
\end{example}

The multi-step evaluation relation $↝^{*}$ explains how STLC programs are executed.
We call such an explanation an \emph{operational semantics}.
The particular way in which we defined $↝^{*}$---first describe single computation steps with $↝$, then take the reflexive-transitive closure---is called \emph{small-step operational semantics}.
An alternative approach, which describes the result of a whole program at once (and which we will not discuss here), is called \emph{big-step operational semantics}.

From the small-step semantics, we obtain a notion of when two programs are equivalent.

\begin{definition}[β-equivalence]
  Two terms $t$ and $u$ are \emph{β-equivalent} if $t ↝^{*} u$ or $u ↝^{*} t$.
\end{definition}

In other words, β-equivalence is the symmetric closure of $↝^{*}$, and hence the equivalence closure of $↝$.
Two β-equivalent terms $t$ and $u$ are also called \emph{convertible}.

\begin{definition}[β-redex]
  A term $t$ is a \emph{β-redex} if $t = (\Lam{x : T}{u})~v$ for some $x,T,u,v$.
\end{definition}

In other words, a term is a β-redex if it could be reduced by an application of the β rule.

\begin{definition}[β-normal form]
  A term $t$ is \emph{β-normal} or in \emph{β-normal form} if there is no term $u$ such that $t ↝ u$.
\end{definition}

Equivalently, a β-normal term is fully evaluated, meaning it does not contain any β-redexes as subterms.

\subsection{λL}

In this section, we extend STLC with additional types: products, sums, the empty type and the unit type.
As we shall see, these correspond to fundamental logical constructions, so we call the resulting system λL.

\begin{definition}[Types]
  The types of $λL$ are generated by the following grammar, where $B$ is a base type.

  \[
    T,U ∷= B \mid T → U \mid ⊤ \mid ⊥ \mid T × U \mid T + U
  \]
\end{definition}

Base types and function types are familiar from STLC.\@
$⊤$ is the type with one value.
$⊥$ is the type with no values.
$T × U$ is the (Cartesian) \emph{product} of $T$ and $U$.
$T + U$ is the \emph{disjoint union} of $T$ and $U$, also called the \emph{sum} or \emph{tagged union} or \emph{discriminated union}.
In Haskell, $T + U$ is spelled $\mathrm{Either}~T~U$.

Note that the grammar which we have used to define the λL types is merely yet another notation for an inductively defined set.
The productions can be read as rules: if $T$ and $U$ are types, then $T → U$ is a type, $T × U$ is a type, etc.

\begin{definition}[Contexts]
  As for STLC, λL contexts are partial maps from variables to λL types.
\end{definition}

\begin{definition}[Terms]
  The terms of $λL$ are generated by the following grammar, where $x$ is a variable.
  \begin{eqnarray*}
    t,u,v &\Coloneqq& x \mid t~u \mid \Lam{x : T}{t} \\
          &\mid& \unit \\
          &\mid& \absurd_{T}~t \\
          &\mid& (t,u) \mid π₁~t \mid π₂~t \\
          &\mid& \inl_{T}~t \mid \inr_{T}~t \mid \case_{T}~t~u~v
  \end{eqnarray*}
\end{definition}

Variables, applications and abstractions are familiar from STLC.\@
The new terms have the following intuitive meaning.

\begin{itemize}
  \item The constant $\unit$ is the unique value of the unit type $⊤$.
  \item Given $t : ⊥$, $\absurd_{T}~t$ has type $T$ for any type $T$.
        This is safe because we do not add any terms of type $⊥$, so there is no closed term of type $⊥$.
        However, we can still have an assumption $t :  ⊥$ in the context, and then $\absurd_{T}$ can be applied.
        As we shall see, this somewhat unintuitive setup has important logical meaning.
  \item Given $t : T$ and $u : U$, the term $(t, u) : T × U$ is the pair of $t$ and $u$.
  \item Given a pair $(t, u) : T × U$, $π₁~(t, u)$ evaluates to the first component $t : T$ and $π₂~(t, u)$ evaluates to the second component $u : U$.
        In other words, $π₁$ is the first (or left) projection and $π₂$ the second (or right).
  \item Given $t : T$, $\inl_{U}~t : T + U$ is the canonical injection of $T$ into $T + U$.
        It is analogous to the constructor \texttt{Left} of Haskell's \texttt{Either} type.
        Similarly, given $u : U$, $\inr_{T}~u$ is analogous to Haskell's \texttt{Right}.
  \item Given $w : T + U$, $l : T → V$ and $r : U → V$, $\case_{V}~w~l~r$ performs case analysis on its first argument $w$, so it evaluates as follows:
        \begin{itemize}
          \item If $w$ evaluates to $\inl_{U}~t$ with $t : T$, then $\case_{V}~w~l~r$ evaluates to $l~t : V$.
          \item If $w$ evaluates to $\inr_{T}~u$ with $u : U$, then $\case_{V}~w~l~r$ evaluates to $r~u : V$.
        \end{itemize}
        This makes $\case_{V}$ similar to Haskell's \texttt{case-of} construct (specialised to the \texttt{Either} type).
\end{itemize}

Note that the new terms can also be understood as constants and functions: $\absurd_{T}$ has type $⊥ → T$, $π₁$ has type $T × U → T$ (for any $T$ and $U$), etc.
We can partition these functions into two classes: \emph{constructors} and \emph{destructors}.
Constructors are the functions which produce one of the new types: $\unit$, $\pair$, $\inl$ and $\inr$.
(We omit the type subscripts when they are clear from context or unimportant.)
Constructors are canonical elements of the new types, in the sense that e.g.\ a program of type $T × U$ is fully evaluated once it is of the form $(t,u)$ and $t$ and $u$ are also fully evaluated.
Dually, destructors, also called \emph{eliminators}, are the functions which consume one of the new types: $\absurd$, $\projl$, $\projr$ and $\case$.
They extract information from these types; e.g.\ $π₁$ and $π₂$ extract the components of a pair.

\begin{definition}[Typing]\label{def:ll-typing}
  The typing relation $Γ ⊢ t : T$ of λL, where $Γ$ is a λL context, $t$ a λL term and $T$ a λL type, is inductively defined by the following rules.

  \begin{mathpar}
    \infr{Var}{Γ(x) = T}{Γ ⊢ x : T}

    \infr{App}{Γ ⊢ f : T → U \\ Γ ⊢ t : T}{Γ ⊢ t~u : U}

    \infr{Abs}{Γ,\, x : T ⊢ t : U}{Γ ⊢ \Lam{x : T}{t} : T → U}

    \infr{Unit}{ }{Γ ⊢ \unit : ⊤}

    \infr{Absurd}{Γ ⊢ t : ⊥}{Γ ⊢ \absurd_T~t : T}

    \infr{Pair}{Γ ⊢ t : T \\ Γ ⊢ u : U}{Γ ⊢ (t, u) : T × U}

    \infr{Proj1}{Γ ⊢ t : T × U}{Γ ⊢ π₁~t : T}

    \infr{Proj2}{Γ ⊢ t : T × U}{Γ ⊢ π₂~t : U}

    \infr{Inl}{Γ ⊢ t : T}{Γ ⊢ \inl_U : T + U}

    \infr{Inr}{Γ ⊢ u : U}{Γ ⊢ \inr_T : T + U}

    \infr{Case}{Γ ⊢ w : T + U \\ Γ ⊢ l : T → V \\ Γ ⊢ r : U → V}{Γ ⊢ \case_V~w~l~r : V}
  \end{mathpar}
\end{definition}

The first three rules are those of STLC.\@
The other rules declare that the new terms of λL have the expected types.
Similarly, the following evaluation relation declares that the new terms have the expected computational behaviour.

\begin{definition}[Small-Step Semantics]
  The λL single-step evaluation relation $↝$ is defined inductively by the following rules.

  \begin{mathpar}
    \infr{β}{ }{(\Lam{x : A}{t})~u ↝ t[x ↦ u]}

    \infr{CongAppL}{t ↝ t'}{t~u ↝ t'~u}

    \infr{CongAppR}{u ↝ u'}{t~u ↝ t~u'}

    \infr{Proj1Pair}{ }{π₁~(t, u) ↝ t}

    \infr{Proj2Pair}{ }{π₂~(t, u) ↝ u}

    \infr{CaseInl}{ }{\case_{V}~(\inl_{U}~t)~l~r ↝ l~t}

    \infr{CaseInr}{ }{\case_{V}~(\inr_{T}~u)~l~r ↝ r~u}

    \infr{CongPair1}{t ↝ t'}{(t, u) ↝ (t', u)}

    \infr{CongPair2}{u ↝ u'}{(t, u) ↝ (t, u')}

    \infr{CongProj1}{t ↝ t'}{π₁~t ↝ π₁~t'}

    \infr{CongProj2}{t ↝ t'}{π₂~t ↝ π₂~t'}

    \infr{CongInl}{t ↝ t'}{\inl_U~t ↝ \inl_U~t'}

    \infr{CongInr}{u ↝ u'}{\inr_T~u ↝ \inr_T~u'}

    \infr{CongCase1}{w ↝ w'}{\case_V~w~l~r ↝ \case_V~w'~l~r}

    \infr{CongCase2}{l ↝ l'}{\case_V~w~l~r ↝ \case_V~w~l'~r}

    \infr{CongCase3}{r ↝ r'}{\case_V~w~l~r ↝ \case_V~w~l~r'}
  \end{mathpar}
\end{definition}

The rules \lbl{β}, \lbl{CongAppL} and \lbl{CongAppR} are again familiar from STLC.\@
\lbl{Proj1Pair}, \lbl{Proj2Pair}, \lbl{CaseInl} and \lbl{CaseInr} are new \enquote*{substantive} rules.
They describe how destructors and constructors interact.
The various \lbl{Cong} rules are congruence rules which, as in STLC, allow us to make an evaluation step in any part of the term.

\subsection{The Curry-Howard Correspondence}

The typing rules of λL given in Definition~\ref{def:ll-typing} bear a striking resemblance to natural deduction rules for propositional logic.
Consider the \lbl{Pair} rule:
\begin{mathpar}
  \infr{Pair}{Γ ⊢ t : T \\ Γ ⊢ u : U}{Γ ⊢ (t, u) : T × U}
\end{mathpar}
If we remove the terms, replace the product type $T × U$ with the conjunction $T ∧ U$ and view $Γ$ as a set of assumptions, the rule becomes
\begin{mathpar}
  \infr{∧-intro}{Γ ⊢ T \\ Γ ⊢ U}{Γ ⊢ T ∧ U}
\end{mathpar}
This is precisely the conjunction introduction rule from propositional logic (presented in natural deduction style): if under assumptions $Γ$ we can prove both $T$ and $U$, then we can also prove $T ∧ U$.

Similarly, we can read $⊤$ as the true proposition, $⊥$ as the false proposition, $T → U$ as \enquote{$T$ implies $U$} and $T + U$ as $T ∨ U$.
All our typing rules then become logical rules:
\begin{itemize}
  \item \lbl{Var} allows us to use assumptions contained in $Γ$.
  \item \lbl{App} is the implication elimination rule (or modus ponens): if $T$ implies $U$ and $T$ is provable, then $U$ is provable.
  \item \lbl{Abs} is implication introduction: if $U$ is provable under the assumption $T$, then $T$ implies $U$.
  \item \lbl{Unit} states that the true proposition is indeed always true.
  \item \lbl{Absurd} states that the false proposition implies anything (\emph{ex falso quodlibet}).
        In other words: if we can prove $⊥$ under assumptions $Γ$, then the assumptions must have already been inconsistent, so we may deduce any proposition.
  \item \lbl{Proj1} is left conjunction elimination: from $T ∧ U$ follows $T$.
        Similarly, \lbl{Proj2} is right conjunction elimination.
  \item \lbl{Inl} is left disjunction introduction: from $T$ follows $T ∨ U$.
        Similarly, \lbl{Inr} is right disjunction introduction.
  \item \lbl{Case} is disjunction elimination: if we know $T ∨ U$ and $T$ implies $V$ and $U$ implies $V$, we may deduce $V$.
\end{itemize}
Note that typing rules for constructors become introduction principles while those for destructors become elimination principles.

Astute readers may have noticed that λL so far lacks negation.
However, this connective can be encoded in λL by defining the type $¬ T$ as $T → ⊥$.
This yields sensible rules for negation:
\begin{itemize}
  \item To prove $¬ T$, we assume $T$ (by rule \lbl{Abs}) and derive a contradiction.
  \item When $¬ T$ is assumed and we can prove $T$, we can conclude $⊥$ (by rule \lbl{App}) and from this follows anything (by rule \lbl{Absurd}).
\end{itemize}

The typing rules of λL in fact not only correspond to some logical rules; they are sound and complete for \emph{intuitionistic propositional logic (IPL)}.
This means that $⊢ T$ is provable in IPL if and only if there is a λL term $t$ such that $⊢ t : ⟨T⟩$, where $⟨T⟩$ is the λL term corresponding to the logical formula $T$.
The formal system λL thus doubles as both a programming language and a logic, a remarkable fact known as the Curry-Howard correspondence.

Unlike the standard \emph{classical propositional logic (CPL)}, IPL is \emph{constructive}.
This means, roughly, that a proof of a proposition $P$ must give concrete evidence of $P$.
For example, a proof of $P ∧ Q$ must prove both $P$ and $Q$; a proof of $P ∨ Q$ must prove either $P$ or $Q$; and so on.
By contrast, CPL's \emph{law of the excluded middle} directly proves, for any proposition $P$, the formula $P ∨ ¬ P$.
This proof clearly does not give concrete evidence for either $P$ or $¬ P$.

The distinction between constructive and classical logic will become most salient in predicate logic, where a constructive proof of the proposition $\Ex{x}{P(x)}$ must give a concrete object $x$ with the desired property $P$.
In contrast, classical predicate logic, which is used for the vast majority of mathematics, allows indirect existence proofs:
to prove $\Ex{x}{P(x)}$, it suffices to prove $¬¬ \Ex{x}{P(x)}$ (by the principle of \emph{double negation elimination}).
Such a proof need not, and usually does not, construct a suitable object $x$.

\subsection{Some Metatheoretic Properties}

We now discuss some metatheoretic properties of λL, omitting most proofs.
The type theories used in theorem provers such as Agda, Coq and Lean are designed to have most of these properties, though with some caveats.
All the properties also hold for STLC since it is a \enquote*{subset} of λL.

We start with uniqueness of typing, which states that a term only has one type.
For STLC, this is quite straightforward.
For more complex theories, uniqueness sometimes has to be weakened to allow a term to have a family of equivalent types (in some sense).

\begin{theorem}[Uniqueness of typing]
  For any context $Γ$, term $t$ and types $T,U$, if $Γ ⊢ t : T$ and $Γ ⊢ t : U$, then $T = U$.
\end{theorem}

Next, decidability of type checking means that there is an algorithm which decides whether a program has a given type (in a given context).
This is essential if we want a computer to be able to typecheck our type theory.

\begin{theorem}[Decidability of type checking]
  For any context $Γ$, term $t$ and type $T$, it is decidable whether $Γ ⊢ t : T$ or $Γ ⊬ t : T$.
\end{theorem}

Subject reduction states that the type of an expression does not change during evaluation.
This may seem like a straightforward requirement, but some popular type theories break subject reduction in subtle (and ultimately harmless) ways.
Subject reduction (or a slightly weaker version) is sometimes called \emph{preservation}.

\begin{theorem}[Subject reduction]\label{th:subject-reduction}
  For any context $Γ$, terms $t,u$ and type $T$, if $Γ ⊢ t : T$ and $t ↝^{*} u$, then $Γ ⊢ u : T$.
\end{theorem}

As an exercise, we prove that subject reduction holds for STLC.\@
Extending the proof to λL is then entirely mechanical.
As is typical, the proof relies crucially on a corresponding lemma about substitution.

\begin{lemma}[Substitution preserves typing]\label{th:subst-typing}
  For any context $Γ$, variable $x$, types $A, T$ and terms $t, a$: if $Γ,\, x : A ⊢ t : T$ and $Γ ⊢ a : A$, then $Γ ⊢ t[x ↦ a] : T$.
\end{lemma}

\begin{proof}
  By induction on $t$.
  \begin{itemize}
    \item $t = x$, hence $t[x ↦ a] = a$.
          From the assumption $Γ,\, x : A ⊢ t : T$ follows $T = A$.
          Since $Γ ⊢ a : A$, we obtain $Γ ⊢ t[x ↦ a] : T$.
    \item $t = y$ with $y ≠ x$, hence $t[x ↦ a] = y$.
          From the assumption $Γ,\, x : A ⊢ t : T$ follows $Γ(y) = T$, so we obtain $Γ ⊢ t[x ↦ a] : T$.
    \item $t = u~v$, hence $t[x ↦ a] = u[x ↦ a]~v[x ↦ a]$.
          From the assumption $Γ,\, x : A ⊢ t : T$ follows $Γ,\, x : A ⊢ u : X → T$ and $Γ,\, x : A ⊢ v : X$ for some $X$.
          With the induction hypothesis for $u$ and $v$, we obtain $Γ ⊢ u[x ↦ a] : X → T$ and $Γ ⊢ v[x ↦ a] : X$, and thus $Γ ⊢ u[x ↦ a]~v[x ↦ a] : T$.
    \item $t = \Lam{y : B}{u}$, hence $t[x ↦ a] = \Lam{y : B}{u[x ↦ a]}$.
          Here we assume, by the Barendregt convention, that $x ≠ y$ and $y ∉ \fv(a)$.
          From the assumption $Γ,\, x : A ⊢ t : T$ follows $T = B → C$ for some $C$ and $Γ,\, x : A,\, y : B ⊢ u : C$.
          Since $x ≠ y$, this is equivalent to $Γ,\, y : B,\, x : A ⊢ u : C$.
          With the induction hypothesis for $u$, we then obtain $Γ,\, y : B ⊢ u[x ↦ a] : C$ and hence $Γ ⊢ \Lam{y : B}{u[x ↦ a]} : B → C$.
  \end{itemize}
\end{proof}

The proof proceeds by \emph{structural induction}, a proof technique used pervasively in type theory (as well as logic, programming language research and other related fields).
The technique is applicable to any inductively defined set $S$.
When proving a property $P(x)$ for all members $x$ of $S$, we first perform a case distinction according to the rules that inductively generate $S$.
For instance, in the previous proof, we must distinguish between terms that are variables, applications and abstractions, mirroring Definition~\ref{def:stlc-terms}.
(The variable case is then immediately split into two, but this is not part of the structural induction technique.)
Further, in the case for a rule $R$ with premises $y₁, \dots, yₙ ∈ S$, we may assume that $P(y₁), \dots, P(yₙ)$ have already been proven.
For instance, the application case above corresponds to the rule $u ∈ \Terms ∧ v ∈ \Terms ⇒ u~v ∈ \Terms$, so we may assume $P(u)$ and $P(v)$, where $P(u)$ translates to the proposition
\[
  ∀\, Γ, x, A, T.\, (Γ,\, x : A ⊢ u : T) ∧ (Γ ⊢ a : A) ⇒ Γ ⊢ u[x ↦ a] : T
\]
We call $P(u)$ the induction hypothesis for $u$, and similar for $v$.

The structural induction technique is justified in much the same manner as the familiar induction on natural numbers.
We know that any member $x ∈ S$ is built from finitely many applications of the rules that generate $S$.
Hence, to prove $x$, we may first prove $P(y₁), \dots, P(yₙ)$ for the premises of the rule was used to generate $x$.
Similarly, to prove $P(yᵢ)$, we may first prove $P$ for the premises of the rule that was used to generate $yᵢ$, and so on until we reach an element generated by a rule without premises (as we must, since the depth of rule applications is finite).
So when we prove $P(x)$, we may assume that $P(yᵢ)$ has already been proved for every $i$.

Now we can prove subject reduction for the single-step reduction relation.

\begin{lemma}[Single-step subject reduction]\label{lem:subject-reduction-single}
  For any context $Γ$, terms $t,u$ and type $T$, if $Γ ⊢ t : T$ and $t ↝ u$, then $Γ ⊢ u : T$.
\end{lemma}

\begin{proof}
  By induction on the derivation of $t ↝ u$ (defined in Definition~\ref{def:single-step}).
  \begin{itemize}
    \item Case \lbl{β}, i.e.\ $t = (\Lam{x : X}{v})~w$ and $u = v[x ↦ w]$.
          From $Γ ⊢ t : T$ follows $Γ ⊢ w : X$ and $Γ ⊢ \Lam{x : X}{v} : X → T$, thus $Γ,\, x : X ⊢ v : T$.
          From this and Lemma~\ref{th:subst-typing} follows $Γ ⊢ v[x ↦ w] : T$.
    \item Case \lbl{CongAppL}, i.e.\ $t = v~w$ and $u = v'~w$ with $v ↝ v'$.
          From $Γ ⊢ t : T$ follows $Γ ⊢ v : A → T$ for some $A$ and $Γ ⊢ w : A$.
          By the induction hypothesis for $v ↝ v'$, this implies $Γ ⊢ v' : A → T$ and thus $Γ ⊢ v'~w : T$.
    \item Case \lbl{CongAppR}, i.e.\ $t = v~w$ and $u = v~w'$ with $w ↝ w'$.
          From $Γ ⊢ t : T$ follows $Γ ⊢ v : A → T$ for some $A$ and $Γ ⊢ w : A$.
          By the induction hypothesis for $w ↝ w'$, this implies $Γ ⊢ w' : A$ and thus $Γ ⊢ v~w' : T$.
  \end{itemize}
\end{proof}

This proof demonstrates another variation on the structural induction technique: it can also be used with inductively defined relations and predicates.
Additionally, the proof uses \emph{inversion} of inductively defined relations pervasively (and implicitly).
Inversion refers to the observation that any derivation in the relation must ultimately be made via one of the relation's defining rules.
For example, in the \lbl{CongAppL} case, we know that $Γ ⊢ v~w : T$.
This derivation must have been made with one of the rules of the typing relation (Definition~\ref{def:stlc-typing}).
Moreover, the typing relation actually only has one rule, \lbl{App}, that can be used for a term of the form $v~w$.
Hence the \lbl{App} rule must have been used and so its premises must be true as well: $Γ ⊢ v : A → T$ for some $A$ and $Γ ⊢ w : A$.

Multi-step subject reduction now follows directly from single-step subject reduction.

\begin{proof}[Proof of Theorem~\ref{th:subject-reduction}]
  By induction on the derivation of $t ↝^{*} u$ (defined in Definition~\ref{def:refl-trans-clos}).
  \begin{itemize}
    \item Case \lbl{Step}, i.e.\ $t ↝ u$.
          Then the result follows from Lemma~\ref{lem:subject-reduction-single}.
    \item Case \lbl{Refl}, i.e.\ $t = u$.
          Obvious.
    \item Case \lbl{Trans}, i.e.\ $t ↝^{*} v$ and $v ↝^{*} u$.
          From the induction hypothesis for $t ↝^{*} v$ we obtain $Γ ⊢ v : T$.
          And from the induction hypothesis for $v ↝^{*} u$ follows $Γ ⊢ u : T$.
  \end{itemize}
\end{proof}

The next four results concern the reduction relation.
First, the confluence theorem states that if we start to evaluate a term $t$ in two different ways, obtaining terms $u₁$ and $u₂$, then we can continue the evaluation of both $u₁$ and $u₂$ in such a way that we end up with the same term $v$.
This is also called the diamond property.
The term \enquote{Church-Rosser property} refers either to confluence or to a slightly different but equivalent property.

\begin{theorem}[Confluence/Church-Rosser/Diamond property]\label{th:confluence}
  For any well-typed term $t$, if $t ↝^{*} u₁$ and $t ↝^{*} u₂$ for some terms $u₁$ and $u₂$, then there is a term $v$ such that $u₁ ↝^{*} v$ and $u₂ ↝^{*} v$.
\end{theorem}

Confluence immediately implies that if a term has a β-normal form, this form is unique.

\begin{corollary}[Uniqueness of normal forms]\label{th:nf-unique}
  For any well-typed term $t$ and β-normal terms $u$ and $v$, if $t ↝^{*} u$ and $t ↝^{*} v$, then $u ≡_{α} v$.
\end{corollary}

\begin{proof}
  Suppose $t ↝^{*} u$ and $t ↝^{*} v$.
  By confluence, there is a term $w$ such that $u ↝^{*} w$ and $v ↝^{*} w$.
  But since $u$ is β-normal, it cannot be the case that $u ↝ w$, so $u$ and $w$ must already be α-equivalent, and similar for $v$.
  Hence $u ≡_{α} w ≡_{α} v$.
\end{proof}

Next, weak normalisation states that any well-typed term can be evaluated to a normal form.
In other words, for any well-typed term there is a β-normalising reduction sequence.
However, if we apply the wrong evaluation steps, we may

\begin{theorem}[Weak normalisation]\label{th:wn}
  For any well-typed term $t$, there is a β-normal term $u$ such that $t ↝^{*} u$.
\end{theorem}

Strong normalisation strengthens the weak normalisation property by requiring that \emph{all} reduction sequences are β-normalising.
This means that we can apply the evaluation rules in any order to reach a β-normal form, which is unique by Theorem~\ref{th:nf-unique}.

\begin{theorem}[Strong normalisation]\label{th:sn}
  For any well-typed term $t₁$, any sequence of $↝$-reductions $t₁ ↝ t₂ ↝ \dots$ is finite.
\end{theorem}

When viewing λL as a programming language, strong normalisation tells us that all programs in this language terminate, as any sequence of evaluation steps eventually halts.
The language can therefore not be Turing-complete.

However, in practice, we not only want our programs to terminate but also to compute sensible values.
So a closed program of type $T × U$ should compute a pair of values of $T$ and $U$; a closed program of type $T + U$ should compute a value of the form $\inl_{U}~t$ or $\inl_{T}~u$; and so on.
This means that closed β-normal forms should be \emph{canonical} and if this is the case, we say that λL enjoys \emph{canonicity}.

To prove canonicity, we first partition the normal forms into those we consider canonical and those we consider non-canonical.

\begin{definition}[Values]
  A β-normal term $t$ is \emph{canonical} or a \emph{value} if it has one of the following forms.
  \begin{itemize}
    \item $t = \unit$
    \item $t = \Lam{x : U}{v}$ for some $u, U, v$
    \item $t = (u, v)$ for some $u, v$
    \item $t = \inl_{U}~v$ for some $U, v$
    \item $t = \inl_{V}~u$ for some $V, u$
  \end{itemize}
  Non-canonical β-normal terms are called \emph{neutral} or \emph{stuck}.
\end{definition}

It is now straightforward to prove canonicity.

\begin{theorem}[Canonicity]
  For any β-normal term $t$ and type $T$ with $⊢ t : T$, $t$ is a value.
\end{theorem}

\begin{proof}
  By induction on $t$.
  \begin{itemize}
    \item $t = \Lam{x : U}{v}$, $t = \unit$, $t = (u, v)$, $t = \inl_{V}~u$ or $t = \inr_{U}~v$.
          Then $t$ is a value.
    \item $t = x$ with $x$ a variable.
          This is impossible since no variable is well-typed in the empty context.
    \item $t = \absurd_{T}~u$ with $⊢ u : ⊥$.
          Since subterms of β-normal terms are also β-normal, $u$ is β-normal.
          Hence, by the induction hypothesis, $u$ is a value.
          However, there is no value of type $⊥$, so this is impossible.
    \item $t = u~v$.
          By the induction hypothesis, $u$ must also be a value.
          But then $u$ must be of the form $\Lam{x : V}{w}$ in order for $t$ to be well-typed, so $t$ is not β-normal.
          So this case is also impossible.
    \item $t = π₁~u$ or $t = π₂~u$.
          Similar to the previous case, $u$ must be of the form $(v, w)$, so $t$ is not β-normal.
    \item $t = \case_{V}~u~l~r$.
          Similar to the previous case, $u$ must be of the form $\inl_{W}~v$ or $\inr_{W}~v$, so $t$ is not β-normal.
  \end{itemize}
\end{proof}

From canonicity immediately follows another very important property, \emph{consistency}.
It states that there is no closed term of type $⊥$ or, equivalently, that the false proposition is not provable in λL.
This property makes λL viable as a logic.

\begin{theorem}[Consistency]\label{th:consistency}
  There is no term $t$ such that $⊢ t : ⊥$.
\end{theorem}

\begin{proof}
  Suppose there is such a term $t$.
  Then, by strong normalisation (Theorem~\ref{th:sn}), there is a β-normal term $t'$ with $t ↝^{*} t'$ and thus, by subject reduction (Theorem~\ref{th:subject-reduction}), $⊢ t' : ⊥$.
  By canonicity, $t'$ is a value, but there is no value of type $⊥$, so we have a contradiction.
\end{proof}

Some authors use slightly different notions of consistency, but these are implied by the previous theorem.
Canonicity is sometimes taken to mean that every term reduces to a value, but this is an immediate consequence of strong normalisation and our notion of canonicity.

\subsection{Denotational Semantics}

We have so far given meaning to STLC and λL operationally, by explaining how their terms evaluate as programs.
Another approach is to translate our lambda calculi into appropriate mathematical structures.
Such a translation is called a \emph{denotational semantics} or \emph{model} and different models can help us prove different properties of the calculi, though we will only see one example of this.

In fact, we have informally been doing this since we defined STLC, viewing $T → U$ as a function space, $\Lam{x : T}{t}$ as a function, and so on.
In this section, we make these intuitions precise.

\begin{definition}[Full set-theoretic model of λL]
  We assume an interpretation function $⟦·⟧_{\Base}$ which maps every base type $B ∈ \Base$ to a set $⟦B⟧_{\Base}$.
  The \emph{full set-theoretic model} of λL is defined by three \emph{interpretation functions}:
  \begin{itemize}
    \item $⟦·⟧_{\Types}$ maps each type $T ∈ \Types$ to a set $⟦T⟧_{\Types}$.
    \item $⟦·⟧_{\Ctxs}$ maps each $Γ ∈ \Ctxs$ to a set $⟦Γ⟧_{\Ctxs}$.
    \item $⟦·⟧_{\Terms}^{Γ,T}$ maps each term $t$ with $Γ ⊢ t : T$ to a function $⟦t⟧_{\Terms}^{Γ,T} : ⟦Γ⟧_{\Ctxs} → ⟦T⟧_{\Types}$.
          (Technically, this is a family of functions.)
  \end{itemize}

  We henceforth omit the subscripts of the interpretation functions since they will always be clear from context.
  For terms, we write $⟦Γ ⊢ t : T⟧$ instead of $⟦t⟧_{\Terms}^{Γ,T}$.

  The interpretation function for types is defined by recursion on the interpreted type:
  \begin{align*}
    ⟦B⟧ &≔ ⟦B⟧_{\Base} \qquad \text{if $B$ is a base type} \\
    ⟦⊤⟧ &≔ \{∅\} \\
    ⟦⊥⟧ &≔ ∅ \\
    ⟦T × U⟧ &≔ ⟦T⟧ × ⟦U⟧ \\
    ⟦T + U⟧ &≔ ⟦T⟧ ⊎ ⟦U⟧
  \end{align*}

  The interpretation function for contexts maps each context to the set of possible assignments to its variables:
  \[
    ⟦Γ⟧ ≔ \left\{ γ : \dom(Γ) → ⋃_{x ∈ \dom(Γ)} ⟦Γ(x)⟧ \;\bigg|\; γ(x) ∈ ⟦Γ(x)⟧\; ∀\, x ∈ \dom(Γ) \right\}
  \]

  The interpretation function for terms is defined by recursion on the derivation of $Γ ⊢ t : T$.
  Recall that this is a function which takes $γ ∈ ⟦Γ⟧$ as an argument and constructs a member of the set $⟦T⟧$.
  \begin{align*}
    ⟦Γ ⊢ x : T⟧(γ) &≔ γ(x) \\
    ⟦Γ ⊢ t~u : T⟧(γ) &≔ ⟦Γ ⊢ t : U → T⟧(γ)(⟦Γ ⊢ u : U⟧(γ)) \\
    ⟦Γ ⊢ \Lam{x : T}{u} : T → U⟧(γ) &≔ t ↦ ⟦Γ,\, x : T ⊢ u : U⟧(γ ∪ \{x ↦ t\}) \\
    ⟦Γ ⊢ \unit : ⊤⟧(γ) &≔ ∅ \\
    ⟦Γ ⊢ \absurd_{T}~t : T⟧(γ) &~\text{[impossible because $⟦Γ ⊢ t : ⊥⟧(γ) ∈ ∅$]} \\
    ⟦Γ ⊢ (t, u) : T × U⟧(γ) &≔ (⟦Γ ⊢ t : T⟧(γ), ⟦Γ ⊢ u : U⟧(γ)) \\
    ⟦Γ ⊢ π₁~t : T⟧(γ) &≔ π₁(⟦Γ ⊢ t : T × U⟧(γ)) \\
    ⟦Γ ⊢ π₂~t : U⟧(γ) &≔ π₂(⟦Γ ⊢ t : T × U⟧(γ)) \\
    ⟦Γ ⊢ \inl_{U}~t : T + U⟧(γ) &≔ (⟦Γ ⊢ t : T⟧(γ), 1) \\
    ⟦Γ ⊢ \inr_{T}~u : T + U⟧(γ) &≔ (⟦Γ ⊢ u : U⟧(γ), 2) \\
    ⟦Γ ⊢ \case_{V}~w~l~r : V⟧(γ) &≔ ⟦Γ ⊢ l : T → V⟧(γ)(t) \qquad \text{if ⟦Γ ⊢ w : T + U⟧(γ) = (t, 1)} \\
    ⟦Γ ⊢ \case_{V}~w~l~r : V⟧(γ) &≔ ⟦Γ ⊢ r : U → V⟧(γ)(u) \qquad \text{if ⟦Γ ⊢ w : T + U⟧(γ) = (u, 2)}
  \end{align*}
  The notation $y ∪ \{x ↦ t\}$ in the third equation denotes the extension of $γ$ with the mapping $x ↦ t$.
  This is the function $δ : \dom(Γ) ∪ \{x\} → \bigl(⋃_{y ∈ \dom(Γ)} ⟦Γ(y)⟧\bigr) ∪ ⟦T⟧$ with $δ(x) = t$ and $δ(y) = γ(y)$ for all $y ≠ x$.
\end{definition}

Above, we use the usual set-theoretic encoding of discriminated unions: the discriminated union $A ⊎ B$ is the set $\{ (a, 1) \mid a ∈ A \} ∪ \{ (b, 2) \mid b ∈ B\}$ with the obvious injections from $A$ and $B$ into $A ⊎ B$.
It may be instructive to check that the definition of the term interpretation function is \enquote*{type-correct}, i.e.\ that the right-hand side of each equation is a member of the set corresponding to the correct type.

The given model formalises our intuitions about the various constructions of $λL$: abstractions can be interpreted as functions; the product type can be interpreted as the Cartesian product; and so on.
This validates the design of λL.
Note, in particular, that the interpretation of the empty context, $⟦∅⟧$, is a set of functions with codomain $∅$.
This set has one trivial member, so while $⟦⊢ t : T⟧$ is technically a function into $⟦T⟧$, we can regard it as a subset of $⟦T⟧$.
Hence, closed terms of type $T → U$ are really interpreted by functions, and similar for the other types.

More practically, the existence of the given model directly implies consistency.

\begin{proof}[Alternative proof of Theorem~\ref{th:consistency}]
  Suppose there is a term $t$ with $⊢ t : ⊥$.
  Then $⟦⊢ t : ⊥⟧$ is a function which maps each element of $⟦∅⟧$ (the interpretation of the empty context) to an element of $⟦⊥⟧ = ∅$.
  But as noted above, $⟦∅⟧$ is a set with at least one member, so there can be no function from $⟦∅⟧$ into $∅$.
  This contradicts the initial assumption.
\end{proof}

\section{Dependent Type Theory}

We finally get to dependent type theories.
This is a class of formal systems which can be regarded as generalisations of STLC.\@
They are constructed in roughly similar ways and feature at least dependent function types, but beyond these basic characteristics there is much variation.

\subsection{Overview}

Most type theories share some central characteristics.
On a technical level, we first weaken the distinction between terms and types.
Both are now part of the same grammatical category, so there is no longer a grammar for terms and a separate one for types, but just one grammar for types.
However, the type system ensures that only certain terms are used as types: while $t : ℕ$ may or may not be provable for a certain $t$, $t : 0$ should never be provable.

Collapsing the distinction between terms and types allows us to have terms in types.
The most characteristic use of this newfound power is the introduction of \emph{Π-types}, also called \emph{dependent function types}.
The type $\PiT{x : T}{U[x]}$ is the type of functions whose codomain $U$ can depend on the function's input $x$ (of type $T$).
This means that $U$ may contain $x$ as a free variable, which we indicate with the notation $U[x]$.
For instance, suppose $\Vec_{A}~n$ is the type of lists with elements of type $A$ and length $n$.
Then the $\tail$ function, which extracts the tail (all elements but the first) from a non-empty list, has type $\PiT{n : ℕ}{\PiT{xs : \Vec_{A}~(n + 1)}{\Vec_{A}~n}}$.
This type indicates that $\tail$ takes as arguments a natural number $n$ and a list of length $n + 1$, returning a list of length $n$.
Note the two crucial ways in which Π-types generalise function types: the term $n$, a variable of type $ℕ$, appears in the type; and the codomain of the function, $\Vec_{A}~n$, depends on the function's input $n$.

As with regular function types, we simulate functions with multiple arguments via currying: the type of $\tail$ is a Π-type whose codomain is another Π-type, but we read it as a function type with two arguments.
Accordingly, we abbreviate $\PiT{x : T}{\PiT{y : U}{V}}$ as $\PiT{(x : T)~(y : U)}{V}$.

The inhabitants of Π-types are lambda abstractions as in STLC, so the $\tail$ function could be defined as $\Lam{n : ℕ}{\Lam{xs : \Vec_{A}~(n + 1)}{t}}$ for some $t$.
In fact, lambda abstractions with Π-types operationally behave exactly like lambda abstractions with regular function types, so they β-reduce by replacing a bound variable with the function's input.
Hence, regular function types emerge as a special case of Π-types: we define the type $T → U$ as $\PiT{x : T}{U}$ when $x$ does not occur freely in $U$.

Logically, Π-types correspond to universal quantification.
Thus, if $P$ is a predicate over natural numbers, then $\PiT{n : ℕ}{P~n}$ is the proposition that $P$ holds for all natural numbers $n$.
Emphasising this intuition, Π-types are also sometimes written as $\All{x : T}{U[x]}$.
Another increasingly popular syntax is $(x : T) → U$.

We further add \emph{Σ-types}, which are in some sense dual to Π-types.
The Σ-type $\SigT{x : T}{U[x]}$ is the type of pairs $(a, b)$ with $a : T$ and $b : U[x ↦ a]$.
In other words, the type of the second component of such a pair may depend on the first component.
For example, the type $\Vec_{A}~n$ can be defined as $\SigT{l : \List~A}{\length~l = n}$.
Values of $\Vec_{A}~n$ are then pairs $(l, p)$ where $l$ is a list and $p$ is a proof that the length of $l$ is $n$.

As with Π-types and functions, the regular product type $T × U$ emerges as a special case of Σ-types: $T × U$ is defined as $\SigT{x : T}{U}$ if $x$ does not occur freely in $U$.
In other words, product types are nondependent Σ-types.
(Despite this, and somewhat confusingly, Σ-types are also called \emph{dependent sum types} and Π-types are also called \emph{dependent product types}.)

Logically, Σ-types correspond to existential quantification.
Thus, $\SigT{n : ℕ}{P~n}$ is the proposition that there exists a natural number $n$ such that the predicate $P$ holds for $n$.
Consequently, Σ-types are sometimes also written as $\Ex{x : T}{U[x]}$

We further introduce \emph{identity types} into the theory.
As its notation suggests, the identity type $a =_{A} b$ represents the proposition that $a$ and $b$ are equal terms of the same type $A$.
If the type $a =_{A} b$ is inhabited (i.e., the proposition can be proved), we say that $a$ and $b$ are \emph{propositionally equal}.
For instance, the type $2 + 3 =_{ℕ} 5$ is inhabited while $2 + 3 =_{ℕ} 4$ is uninhabited (though still a perfectly good type).

Since types now contain terms, computation can happen at the type level: the type $\Vec_{A}~(n + 0)$ reduces to the type $\Vec_{A}~n$, given a suitable definition of addition.
It is often convenient to use this sort of reduction during typechecking, where we want a term of type $\Vec_{A}~n$ to also count as a term of $\Vec_{A}~(n + 0)$ (and $\Vec_{A}~(n + 0 + 0)$, etc.).
We therefore introduce \emph{judgemental} or \emph{definitional} equality, written $a ≡ b$ (where $a$ and $b$ are terms of the same type), together with a typing rule that allows the type checker to treat any term of type $T$ as a term of type $U$ when $T ≡ U$.

In the \emph{intensional} variant of type theory, which we will focus on, two terms are judgementally equal if and only if they reduce to the same β-normal form.
Since we ensure that the type theory is strongly normalising, the β-normal form of a term is computable and so judgemental equality is decidable.
By contrast, \emph{extensional} type theory adds a rule which concludes $T ≡ U$ from a proof of the propositional equality $T = U$.
Since propositional equality is decidedly not decidable, this renders judgemental equality, and hence typechecking, undecidable as well, but the resulting system is arguably more elegant.
Perhaps surprisingly, there are also practical implementations of extensional type theories, such as F*~\cite{DBLP:conf/popl/SwamyHKRDFBFSKZ16}.
These languages perform incomplete, \enquote*{best-effort} typechecking, which is sufficient for many practical applications.

A final new ingredient in the theory are \emph{dependent eliminators}.
We have already seen nondependent eliminators in λL: $\absurd$ for the $⊥$ type and $\case$ for sum types.
Dependent eliminators generalise these and are best understood as induction principles.
For example, consider the function corresponding to $\case$:
\[
  \begin{array}{ll}
    \multicolumn{2}{l}{\case_{A,B,C} : (A → C) → (B → C) → A + B → C} \\
    \case_{A,B,C}~f~g~(\inl_{B}~a) &= f~a \\
    \case_{A,B,C}~f~g~(\inr_{A}~b) &= g~b
  \end{array}
\]
The corresponding dependent eliminator, which we call $\elimSum$, is obtained by making the codomain $C$ depend on the input of type $A + B$.
This means that $C$ has type $A + B → \Univ$, where $\Univ$ is the type of types (though this will be slightly more complex in the full theory).
Hence, $C$ is a function that associates a type to each member of $A + B$.
The dependent eliminator then has the following type and definitional equations:
\[
  \begin{array}{ll}
    \multicolumn{2}{l}{\elimSum_{A,B,C} : (\PiT{x : A}{C~(\inl_{B}~x)}) → (\PiT{y : B}{C~(\inr_{A}~y)}) → \PiT{t : A + B}{C~t}} \\
    \elimSum_{A,B,C}~f~g~(\inl_{B}~a) &= f~a \\
    \elimSum_{A,B,C}~f~g~(\inr_{A}~b) &= g~b
  \end{array}
\]
The equations are exactly identical, so the dependent eliminator has the same computational behaviour as the nondependent one.
The generalised type is best understood in the logical interpretation, where we view $C : A + B → \Univ$ as a predicate on $A + B$: if $C~t$ is inhabited for some $t : A + B$, then $t$ satisfies $C$.
The eliminator's type then reads: if $\inl_{B}~x$ satisfies $C$ for all $x$, and $\inr_{A}~y$ satisfies $C$ for all $y$, then all members of $A + B$ satisfy $C$.
In other words, to prove something about all members of a sum type, it suffices to prove the same statement about the canonical members of the same type, which are applications of its constructors.
This is analogous to the familiar induction principle for natural numbers, which states that to prove something about all natural numbers, it suffices to prove it about zero and the successor of a natural number.

In the successor case of the induction principle for $ℕ$, we may also assume that the statement has already been proved for the previous natural number.
This reflects the fact that $ℕ$ is a recursive (self-referential) type: the successor constructor of $ℕ$ has an argument of type $ℕ$.
By contrast, the constructors of $A + B$ do not have arguments of type $A + B$, so $\elimSum$ does not provide similar assumptions.

In MLTT, we add a dependent eliminator for every type and it always corresponds to a similar proposition.
For example:
\begin{itemize}
  \item To prove $C~t$ for all $t : ⊤$, it suffices to prove $C~\unit$.
  \item To prove $C~t$ for all $t : ⊥$, it suffices to prove nothing (since $⊥$ has no constructors).
  \item To prove $C~t$ for all $t : \SigT{x : A}{B}$, it suffices to prove $C~(a, b)$ for all $a : A$ and $b : B[x ↦ a]$.
\end{itemize}
As special cases, Π-types do not have a dedicated eliminator because application already serves as their elimination form.

\subsection{Intensional Martin-Löf Type Theory}

We now consider a particularly prominent type theory: intensional Martin-Löf type theory (MLTT), which lies at the heart of the Agda theorem prover~\cite{norell:thesis} and of the axiomatic formulation of homotopy type theory~\cite{hottbook}.
Since Per Martin-Löf developed multiple type theories, the term MLTT is not exactly well-defined; we choose the theory published in 1975~\cite{mltt75} with an infinite hierarchy of universes.
The presentation is inspired by (but substantially different from) that on the nLab\footnote{\url{https://ncatlab.org/nlab/show/Martin-Löf+dependent+type+theory}}.

The formal system MLTT consists of three judgements:
\begin{itemize}
  \item $Γ~\ctx$, meaning $Γ$ is a well-formed context
  \item $Γ ⊢ a : A$, meaning $a$ is a well-typed term of type $A$ in context $Γ$
  \item $Γ ⊢ a ≡ b : A$, meaning $a$ and $b$ are judgementally equal terms of type $A$ in context $Γ$
\end{itemize}

The first judgement determines the \emph{well-formed} or \emph{valid} contexts.
We did not need such a judgement for STLC because there, all types, and therefore all contexts, are well-formed.
But in a dependent type theory, types can contain terms, so we need to exclude ill-typed terms from our contexts and types.
The second judgement, $Γ ⊢ a : A$, is a typing judgement as in STLC.\@
The third judgement, $Γ ⊢ a ≡ b : A$, determines when two terms $a$ and $b$ of a type $A$ are considered judgementally equal.

The three judgements are mutually inductively defined.
This means that they may refer to each other in a cyclical fashion, e.g.\ a rule with conclusion $Γ~\ctx$ may have a premise $Γ ⊢ a : A$ but a rule with conclusion $Γ ⊢ a : A$ may also have a premise $Γ~\ctx$.
The judgements are therefore defined not one after the other but all at once.
However, derivations of these judgements are still finite trees of rule applications.
In the following, we give the judgements' rules in chunks, grouping together the rules pertaining to each type.

Note that we do not define a reduction relation $↝$ as we did for STLC.\@
The judgemental equality rules could be recast as reduction rules to recover a small-step semantics.

\subsubsection{Terms}

As for STLC and λL, we assume an infinite set $\Vars$ of variables.

\begin{definition}[Terms]
  The terms of MLTT are inductively defined by the following grammar, where $x$ stands for an arbitrary variable and $n$ for a natural number.
  \begin{eqnarray*}
    a, b, c, d, A, B, C
    &\Coloneqq& x \\
    &|& \Univ_{n} \\
    &|& \PiT{x : A}{B} \mid \Lam{x : A}{t} \mid t~u \\
    &|& \SigT{x : A}{B} \mid (t, u) \mid \elimSig_{A,B,C}~a~b \\
    &|& ⊤ \mid \unit \mid \elimTop_{A}~a~b \\
    &|& ⊥ \mid \elimBot_{A}~a \\
    &|& A + B \mid \inl_{B}~a \mid \inl_{A}~b \mid \elimSum_{A,B,C}~a~b \\
    &|& a =_{A} b \mid \refl_{A}~a \mid \elimId_{A,B}~a~b~c \\
    &|& ℕ \mid \zero \mid \suc \mid \elimNat_{A}~a~b~c
  \end{eqnarray*}
\end{definition}

As mentioned, terms and types are now part of the same grammar, so for example the type $⊥$ is syntactically a term.
However, there is still a clear distinction between terms that are used as types and those which are not, so we will continue to refer to $⊤$, $⊥$, $Π$, $Σ$ and $=$ as types.
The terms $\Univ_{0}$, $\Univ_{1}$, \dots, called \emph{universes}, play a special role in this scheme: we will set up the typing relation such that a term $T$ is a type exactly if $Γ ⊢ T : \Univ_{n}$ is derivable for some $n$.

The other terms we have not seen so far are
\begin{itemize}
  \item Eliminators $\elimSig$, $\elimTop$ and so on.
        These are the dependent eliminators mentioned in the overview.
  \item The constructor $\refl$ of the identity type.
  \item The type of natural numbers, $ℕ$, and its constructors $\zero$ (representing the number 0) and $\suc$ (representing the successor function $· + 1$).
\end{itemize}

Substitution is defined as for STLC.\@
The type constructions $\PiT{x : T}{U}$ and $\SigT{x : T}{U}$ are treated like $\Lam{x : T}{t}$ in that they, too, bind variables and we extend the Barendregt convention to them.

\subsubsection{Contexts and Variable Typing}

Conceptually, contexts are partial maps from variables to types, as in STLC.\@
However, most presentations of type theories encode these partial maps as lists of bindings $x : A$, where $x$ is a variable and $A$ is a type.
The type of a variable $y$ in a context $Γ$ is then the type given in the rightmost binding for $y$ in $Γ$.

\begin{definition}[Contexts]
  MLTT contexts are inductively defined by the following grammar:
  \[
    Γ \Coloneqq ∅ \mid Γ,\, x : T
  \]
\end{definition}

Another change from STLC is that in a context $Γ,\, x : A$, the type $A$ may contain the variables bound in $Γ$.
Such contexts of types which may depend on earlier variables are also called \emph{telescopes}.
A context $∅, x₁ : A₁, x₂ : A₂, x₃ : A₃, \dots, xₙ : Aₙ$ is thus well-formed if
\begin{itemize}
  \item $A₁$ is a well-formed type.
  \item $A₂$ is a well-formed type which may contain $x₁$.
        In other words, $A₂$ is a well-formed type in the context $∅, x₁ : A₁$.
  \item $A₃$ is a well-formed type which may contain $x₁$ and $x₂$.
        In other words, $A₃$ is a well-formed type in the context $∅, x₁ : A₁, x₂ : A₂$.
  \item \dots
  \item $Aₙ$ is a well-formed type in the context $∅, x₁ : A₁, \dots, x_{n-1} : A_{n-1}$.
\end{itemize}
The following definition of well-formedness of contexts captures this requirement.

\begin{definition}[Well-Formedness of contexts]
  \begin{mathpar}
    \infr{}{ }{∅~\ctx}

    \infr{}{Γ~\ctx \\ Γ ⊢ A : \Univ_n}{Γ,\, x : A~\ctx}
  \end{mathpar}
\end{definition}

Having encoded contexts as lists, we need slightly different rules for variables than in our presentation of STLC.\@

\begin{definition}[Typing of variables]
  \begin{mathpar}
    \infr{Var}{Γ~\ctx \\ Γ ⊢ A : \Univ_n}{Γ,\, x : A ⊢ x : A}

    \infr{Weaken}{Γ~\ctx \\ Γ ⊢ B : \Univ_n \\ Γ ⊢ x : A \\ x ≠ y}{Γ,\, y : B ⊢ x : A}
  \end{mathpar}
\end{definition}

The \emph{weakening} rule \lbl{Weaken} allows us to navigate to the rightmost binding for a variable $x$ in a context, and the \lbl{Var} rule then concludes that $x$ has precisely the type given by that binding.
The premises $Γ~\ctx$, $Γ ⊢ A : \Univ_{n}$ and $Γ ⊢ B : \Univ_{n}$ of \lbl{Var} and \lbl{Weaken} could be omitted.
Their purpose is to ensure that for any $Γ$, $x$ and $A$, $Γ ⊢ x : A$ implies that $Γ$ is a valid context and $A$ a valid type in $Γ$.
We will also add similar premises to the other rules of the typing relation, so for any term $t$, $Γ ⊢ t : A$ will similarly imply that $Γ$ and $A$ are valid.
This setup has the advantage that when we give a rule with premise $Γ ⊢ t : A$, we need not specify any more that $Γ$ and $A$ must also be valid, since this is already implied.

\subsubsection{Π-Types}

\begin{definition}[Rules for Π-types]
  \begin{mathpar}
    \infr{Π-Form}{Γ ⊢ A : \Univ_m \\ Γ,\, x : A ⊢ B : \Univ_n}{Γ ⊢ \PiT{x : A}{B} : \Univ_{\max(m,n)}}

    \infr{Π-Intro}{Γ ⊢ A : \Univ_n \\ Γ,\, x : A ⊢ t : B}{Γ ⊢ \Lam{x : A}{t} : \PiT{x : A}{B}}

    \infr{Π-Elim}{Γ ⊢ t : \PiT{x : A}{B} \\ Γ ⊢ u : A}{Γ ⊢ t~u : B[x ↦ u]}

    \infr{Π-Comp}{Γ,\, x : A ⊢ t : B \\ Γ ⊢ u : A}{Γ ⊢ (\Lam{x : A}{t})~u ≡ t[x ↦ u] : B[x ↦ u]}
  \end{mathpar}
\end{definition}

The first rule for the Π-type, \lbl{Π-Form}, states the conditions under which the Π-type $\PiT{x : A}{B}$ is well-formed in a context $Γ$: $A$ must be a well-formed type in $Γ$ and $B$ must be a well-formed type in $Γ,\, x : A$.
The latter means that $B$ may contain the variable $x$.

The second rule, \lbl{Π-Intro}, is the typing rule for abstraction.
It is essentially unchanged from STLC, though we add a premise $Γ ⊢ A ∶ \Univ_{n}$ to ensure that the Π-type is actually well-formed.

The third rule, \lbl{Π-Elim}, is the typing rule for applications.
It, too, is similar to the corresponding STLC rule, but since the codomain type $B$ depends on the dependent function's input, the application has type $B$ with $u$ substituted for $x$.

The fourth rule, \lbl{Π-Comp}, is the β-rule recast as a judgemental equality: the result of applying an abstraction to an input $u$ is the abstraction's body $t$ with the bound variable $x$ replaced by $u$.
Note that unlike in our presentation of STLC, judgemental equality rules are typed, so we add premises to ensure that the terms on both sides of the equation have the right type.
In other words, we design our judgemental equality rules such that $Γ ⊢ t ≡ u : T$ implies $Γ ⊢ t : T$ and $Γ ⊢ u : T$ (and hence $Γ$ is a valid context and $T$ is a valid type in $Γ$).

Compared to STLC and λL, we use a more systematic naming scheme for the rules.
The rule \lbl{Π-Form}, which establishes the well-formed Π-types, is called a \emph{formation rule}.
The rule \lbl{Π-Intro} establishes the type of the constructor, or \emph{introduction form}, of Π-types and is therefore called an \emph{introduction rule}.
The constructor is the canonical way to construct a Π-type, which is the lambda abstraction.
Dually, the rule \lbl{Π-Elim} establishes the type of the eliminator of Π-types and is therefore called an \emph{elimination rule}.
The eliminator is the canonical way to eliminate (or destruct, or use) a Π-type, which is the application.
Finally, the rule \lbl{Π-Comp} is the \emph{computation rule} for Π-types.
It establishes a definitional equality for an application of the eliminator to the constructor.

As mentioned, the function type $T → U$ is defined as the non-dependent Π-type $\PiT{x : T}{U}$, where $x$ does not occur freely in $U$.

\subsubsection{Σ-Types}

The rules for Σ-types also follow the formation/introduction/elimination/computation scheme (as will the rules for all other types).
\begin{definition}[Rules for Σ-types]
  \begin{mathpar}
    \infr{Σ-Form}{Γ ⊢ A : \Univ_m \\ Γ,\, x : A ⊢ B : \Univ_n}{Γ ⊢ \SigT{x : A}{B} : \Univ_{\max(m, n)}}

    \infr{Σ-Intro}{Γ,\, x : A ⊢ B : \Univ_n \\ Γ ⊢ t : A \\ Γ ⊢ u : B[x ↦ t]}{Γ ⊢ (t, u) : \SigT{x : A}{B}}

    \infr{Σ-Elim}{Γ ⊢ P : (\SigT{x : A}{B~x}) → \Univ_n \\ Γ ⊢ p : \PiT{(x : A)~(y : B~x)}{P~(x,y)} \\ Γ ⊢ t : \SigT{x : A}{B~x}}%
      {Γ ⊢ \elimSig_{A,B,P}~p~t : P~t}

    \infr{Σ-Comp}{Γ ⊢ P : (\SigT{x : A}{B~x}) → \Univ_n \\ Γ ⊢ p : \PiT{(x : A)~(y : B~x)}{P~(x,y)} \\ Γ ⊢ t : A \\ Γ ⊢ u : B~t}%
      {Γ ⊢ \elimSig_{A,B,P}~p~(t, u) ≡ p~t~u : P~(t, u)}
  \end{mathpar}
\end{definition}
Note that in \lbl{Σ-Form} and \lbl{Σ-Intro}, $B$ is a type family, i.e.\ a term of type $\Univ_{n}$ with a free variable $x : A$, while in \lbl{Σ-Elim} and \lbl{Σ-Comp} it is a type function, i.e.\ a term of type $A → \Univ_{n}$.
The two are obviously closely related: any type family $B[x]$ induces a function $\Lam{x : A}{B}$ and any function of type $A → \Univ_{n}$ is judgementally equal to $\Lam{x : A}{B[x]}$ for some $B[x]$.
We could also use a type family $B[x]$ as an index for $\elimSig$, but then we would have to specify explicitly the variable $x$.

As mentioned, the product type $T × U$ is defined as the non-dependent sigma type $\SigT{x : T}{U}$ where $x$ does not occur freely in $U$.
In λL, we used note one but two eliminators for product types, the projections $π₁$ and $π₂$.
Reassuringly, we can define these (with suitably generalised types) in terms of $\elimSig$.

\begin{definition}[Projections]
  The first and second projection functions, $π₁$ and $π₂$, are defined by
  \begin{align*}
    π₁ &: \PiT{(A : \Univ_{m})~(B : A → \Univ_{n})}{(\SigT{x : A}{B~x}) → A} \\
    π₁ &≔ \Lam{A~B~t}{\elimSig_{A,B,\Lam{t}{A}}~(\Lam{(x : A)~(y : B~x)}{x})~t} \\
    π₂ &: \PiT{(A : \Univ_{m})~(B : A → \Univ_{n})~(t : \SigT{x : A}{B~x})}{B~(π₁~t)} \\
    π₂ &≔ \Lam{A~B~t}{\elimSig_{A,B,\Lam{(t : \SigT{x : A}{B~x})}{B~(π₁~t)}}~(\Lam{(x : A)~(y : B~x)}{y})~t}
  \end{align*}
\end{definition}
Since the arguments $A$ and $B$ of $π₁$ and $π₂$ are usually clear from context, we omit them.
From the \lbl{Σ-Comp} rule follow the expected judgemental equations for the projections:
\begin{mathpar}
  \infr{$π_1$-Comp}{Γ ⊢ t : A \\ Γ,\, x : A ⊢ u : B~x}{Γ ⊢ π₁~(t, u) ≡ t : A}

  \infr{$π_2$-Comp}{Γ ⊢ t : A \\ Γ,\, x : A ⊢ u : B~x}{Γ ⊢ π₂~(t, u) ≡ u : B~t}
\end{mathpar}

\subsubsection{⊤}

The rules for $⊤$ are analogous to those in λL, but we add the eliminator $\elimTop$ and its computation rule.
The eliminator is not particularly useful in practice, but it still represents a natural induction principle for $⊤$.

\begin{definition}[Rules for ⊤]
  \begin{mathpar}
    \infr{⊤-Form}{Γ~\ctx}{Γ ⊢ ⊤ : \Univ₀}

    \infr{⊤-Intro}{Γ~\ctx}{Γ ⊢ \unit : ⊤}

    \infr{⊤-Elim}{Γ ⊢ P : ⊤ → \Univ_n \\ p : P~\unit}{Γ ⊢ \elimTop_P~p~t : P~t}

    \infr{⊤-Comp}{Γ ⊢ P : ⊤ → \Univ_n \\ p : P~\unit}{Γ ⊢ \elimTop_P~p~\unit ≡ p~\unit : P~\unit}
  \end{mathpar}
\end{definition}

\subsubsection{⊥}

The rules for $⊥$ are exactly as in λL, with the only difference being that the eliminator $\elimBot$ (which we previously called $\absurd$) is now dependent.
Since $⊥$ has no constructors, there is neither an introduction nor a computation rule.

\begin{definition}[Rules for ⊥]
  \begin{mathpar}
    \infr{⊥-Form}{Γ~\ctx}{Γ ⊢ ⊥ : \Univ₀}

    \infr{⊥-Elim}{Γ ⊢ P : ⊥ → \Univ_n \\ Γ ⊢ t : ⊥}{Γ ⊢ \elimBot_P~t : P~t}
  \end{mathpar}
\end{definition}

\subsubsection{Sum Types}

The rules for sum types are, again, like those in λL, only with a dependent eliminator.
Note that we have two constructors, and hence two introduction and computation rules.

\begin{definition}[Rules for sum types]
  \begin{mathpar}
    \infr{+-Form}{Γ ⊢ A : \Univ_n \\ Γ ⊢ B : \Univ_m}{Γ ⊢ A + B : \Univ_{\max(m, n)}}

    \infr{+-Intro1}{Γ ⊢ B : \Univ_n \\ Γ ⊢ t : A}{Γ ⊢ \inl_B~t : A + B}

    \infr{+-Intro2}{Γ ⊢ A : \Univ_n \\ Γ ⊢ u : B}{Γ ⊢ \inr_A~u : A + B}

    \infr{+-Elim}{Γ ⊢ P : A + B → \Univ_n \\ Γ ⊢ p : \PiT{x : A}{P~(\inl_B~x)} \\ Γ ⊢ q : \PiT{y : B}{P~(\inr_A~y)} \\ Γ ⊢ t : A + B}%
      {Γ ⊢ \elimSum_{A,B,P}~p~q~t : P~t}

    \infr{+-Comp1}{Γ ⊢ P : A + B → \Univ_n \\ Γ ⊢ p : \PiT{x : A}{P~(\inl_B~x)} \\ Γ ⊢ q : \PiT{y : B}{P~(\inr_A~y)} \\ Γ ⊢ a : A}%
      {Γ ⊢ \elimSum_{A,B,P}~p~q~(\inl_B~a) ≡ p~a : P~(\inl_B~a)}

    \infr{+-Comp2}{Γ ⊢ P : A + B → \Univ_n \\ Γ ⊢ p : \PiT{x : A}{P~(\inl_B~x)} \\ Γ ⊢ q : \PiT{y : B}{P~(\inr_A~y)} \\ Γ ⊢ b : B}%
      {Γ ⊢ \elimSum_{A,B,P}~p~q~(\inr_A~b) ≡ q~b : P~(\inr_A~b)}
  \end{mathpar}
\end{definition}

\subsubsection{Identity Types}

The rules for identity types follow the same scheme we have seen many times now:
\begin{definition}[Rules for identity types]
  \begin{mathpar}
    \infr{=-Form}{Γ ⊢ A : \Univ_n \\ Γ ⊢ a : A \\ Γ ⊢ b : A}{Γ ⊢ a =_A b : \Univ_n}

    \infr{=-Intro}{Γ ⊢ a : A}{Γ ⊢ \refl_A~a : a =_A a}

    \infr{=-Elim}{Γ ⊢ P : \PiT{(x : A)~(y : A)}{x =_A y → \Univ_n} \\ Γ ⊢ p : \PiT{(x : A)}{P~x~x~(\refl_A~x)} \\ Γ ⊢ a : A \\ Γ ⊢ b : A \\ Γ ⊢ t : a =_A b}%
      {Γ ⊢ \elimId_{A,P}~p~a~b~t : P~a~b~t}

    \infr{=-Comp}{Γ ⊢ P : \PiT{(x : A)~(y : A)}{x =_A y → \Univ_n} \\ Γ ⊢ p : \PiT{(x : A)}{P~x~x~(\refl_A~x)} \\ Γ ⊢ a : A}{Γ ⊢ \elimId_{A,P}~p~a~a~(\refl_A~a) ≡ p~a : P~a~a~(\refl_A~a)}
  \end{mathpar}
\end{definition}
The constructor of identity types, $\refl$, may seem quite restrictive at first glance.
After all, it only allows us to

The type of the eliminator $\elimId$ (usually called the \emph{J rule}) is perhaps less obvious than that of the other eliminators.
It states that to prove a property $P(a, b, p)$ of two equal objects $a$ and $b$ (and of the proof that they are equal, $p : a =_{A} b$), it suffices to prove that $P(a, a, \refl_{A}~a)$.
In other words, we may assume that $a$ and $b$ are actually exactly the same object (and that $p$ is the constructor $\refl_{A}$).

Talking about properties of proofs such as $p$ is not natural from the point of view of regular mathematics.
There, proofs are usually treated as irrelevant: either $a$ and $b$ are equal or they are not, and how the equality is proved does not concern us.
We can also follow this principle and design type theories in which proofs of equality are irrelevant, but in the popular homotopy type theory, it turns out that differences between equality proofs are actually crucial.

The eliminator can be used to prove that identity types, in fact, behave as one would expect from a notion of equality.
In particular, we would expect the identity type over a type $A$ to be an equivalence relation, and indeed, we can show that it is symmetric and transitive (reflexivity is already given by $\refl$).
This amounts to constructing terms of the following types.
\begin{align*}
  \mathrm{sym} &: \PiT{(A : \Univ_{n})~(a : A)~(b : A)}{a =_{A} b → b =_{A} a} \\
  \mathrm{trans} &: \PiT{(A : \Univ_{n})~(a : A)~(b : A)~(c : A)}{a =_{A} b → b =_{A} c → a =_{A} c}
\end{align*}
Further, identity types have the \emph{Leibniz property}: if an object $a$ has some property $P$ and $a$ is equal to $b$, then $b$ also has the property $P$.
This means that $a$ and $b$ are indistinguishable within the theory---a very strong property that makes identity types the strongest possible notion of equality.
Formally:
\[
  \mathrm{leibniz} : \PiT{(A : \Univ_{m})~(P : A → \Univ_{n})~(a : A)~(b : A)}{P~a → a =_{A} b → P~b}
\]

\subsubsection{Natural Number Type}

As an example of a recursive, infinite type, we add the natural numbers.
They are generated by the constructors $\zero$ and $\suc$, where $\suc~x$ represents the natural number $x + 1$.
As mentioned, the eliminator $\elimNat$ is the familiar induction principle for natural numbers.

\begin{definition}[Rules for natural numbers]
  \begin{mathpar}
    \infr{ℕ-Form}{Γ~\ctx}{Γ ⊢ ℕ : \Univ₀}

    \infr{ℕ-Intro1}{Γ~\ctx}{Γ ⊢ \zero : ℕ}

    \infr{ℕ-Intro2}{Γ ⊢ a : ℕ}{Γ ⊢ \suc~a : ℕ}

    \infr{ℕ-Elim}{Γ ⊢ P : ℕ → \Univ_n \\ Γ ⊢ p : P~\zero \\ Γ ⊢ q : \PiT{x : ℕ}{P~x → P~(\suc~x)} \\ Γ ⊢ t : ℕ}%
      {Γ ⊢ \elimNat_P~p~q~t : P~t}

    \infr{ℕ-Comp1}{Γ ⊢ P : ℕ → \Univ_n \\ Γ ⊢ p : P~\zero \\ Γ ⊢ q : \PiT{x : ℕ}{P~x → P~(\suc~x)}}%
      {Γ ⊢ \elimNat_P~p~q~\zero ≡ p : P~\zero}

    \infr{ℕ-Comp2}{Γ ⊢ P : ℕ → \Univ_n \\ Γ ⊢ p : P~\zero \\ Γ ⊢ q : \PiT{x : ℕ}{P~x → P~(\suc~x)} \\ Γ ⊢ n : ℕ}%
      {Γ ⊢ \elimNat_P~p~q~(\suc~n) ≡ q~n : P~(\suc~n)}
  \end{mathpar}
\end{definition}

\subsubsection{Universes}

Each universe $\Univ_{n}$ has type $\Univ_{n + 1}$:
\begin{definition}[Rules for universes]
  \begin{mathpar}
    \infr{U-Form}{Γ~\ctx}{Γ ⊢ \Univ_n : \Univ_{n + 1}}
  \end{mathpar}
\end{definition}
The formation rules for all other types can be read as introduction rules for $\Univ_{n}$.

\subsubsection{Structural Rules for Judgemental Equality}

As defined so far, judgemental equality is quite impoverished---it is not an equivalence relation (e.g., not symmetric) and cannot be used by the typechecker.
We fix this with some additional rules.
These rules are \emph{structural}, which means that, unlike the rules we have seen so far, they are not specific to terms of a certain shape.
\begin{definition}[Judgemental equality rules]
  \begin{mathpar}
    \infr{≡-Refl}{Γ ⊢ a : A}{Γ ⊢ a ≡ a : A}

    \infr{≡-Sym}{Γ ⊢ a ≡ b : A}{Γ ⊢ b ≡ a : A}

    \infr{≡-Trans}{Γ ⊢ a ≡ b : A \\ Γ ⊢ b ≡ c : A}{Γ ⊢ a ≡ c : A}

    \infr{≡-Subst}{Γ,\, x : A ⊢ t : B \\ Γ ⊢ a ≡ b : A}{Γ ⊢ t[x ↦ a] ≡ t[x ↦ b] : B}

    \infr{≡-Type}{Γ ⊢ A ≡ B : \Univ_n \\ Γ ⊢ a : A}{Γ ⊢ a : B}
  \end{mathpar}
\end{definition}

The first three rules make judgemental equality an equivalence relation (compare Def.~\ref{def:equivalence-closure}).

The fourth rule, \lbl{≡-Subst}, allows us to apply a judgemental equality at any location within a term.
For example, if we know $Γ ⊢ t ≡ u : A$ and $Γ ⊢ f t : B$, then from the latter hypothesis follows $Γ,\, x : A ⊢ f x : B$ (where $x$ does not occur in $Γ$, $f$, $A$ or $B$) and hence, by \lbl{≡-Subst}, $Γ ⊢ f t ≡ f u : B$.
In this way, \lbl{≡-Subst} acts as a congruence rule for all term formers of MLTT.\@
The advantage of this setup, compared with explicit congruence rule, is that we only need one rule.
The disadvantage is that \lbl{≡-Subst} is not syntax-directed, which may complicate some metatheoretic proofs.

The fifth rule, \lbl{≡-Type}, is the \emph{raison d'être} of judgemental equality.
It declares that if a term $a$ has type $A$ and $A$ is judgementally equal to $B$, then $a$ also has type $B$.
In other words, the typechecker identifies judgementally equal types.

\subsection{Eta Rules}

In the following sections, we briefly consider some important variations of MLTT.\@
First, MLTT can be extended with \emph{η} or \emph{uniqueness} rules, which introduce additional judgemental equalities.
The most common of these is the η rule for lambda abstractions (often called just \enquote{the η rule}):
\begin{mathpar}
  \infr{Π-η}{Γ ⊢ f : \PiT{x : A}{B} \\ y ∉ \fv(f)}{Γ ⊢ f ≡ \Lam{y : A}{f~y} : \PiT{x : A}{B}}
\end{mathpar}
This rule says that any function $f$ is equal to a lambda abstraction whose body is an application of $f$.
This abstraction is usually called the \emph{η-expansion} of $f$.

Similar η rules can also be added for other types.
For example, here is an η rule for Σ-types:
\begin{mathpar}
  \infr{Σ-η}{Γ ⊢ t : \SigT{x : A}{B}}{Γ ⊢ t ≡ (π₁~t, π₂~t) : \SigT{x : A}{B}}
\end{mathpar}
Like the η rule for Π-types, the η rule for Σ-types says that any term of a Σ-type is judgementally equal to an application of a constructor of Σ-types (the pair $(t, u)$).

It seems clear that these η rules are harmless from a logical point of view.
However, they complicate the metatheory since unlike the other judgemental equalities we have seen, they expand (rather than contract) the terms on their left-hand sides.

\subsection{Universes}

Our presentation of MLTT uses an infinite hierarchy of \emph{universes à la Russel}.
This means that universes are themselves types, and types are members of universes: $⊢ ℕ : \Univ_{0}$, $⊢ ℕ → ℕ : \Univ_{0}$, $⊢ ℕ → \Univ_{0} : \Univ_{1}$, etc.

\emph{Universes à la Tarski} are a common alternative to this setup.
A Tarski universe $\Univ$ does not contain types but rather \emph{codes} for types.
These are set up to mirror the types, so we add, for example, $\bar{ℕ} : \Univ$ as an arbitrary code for $ℕ$, and similarly $A \bar{×} B : \Univ$ whenever $A : \Univ$ and $B : \Univ$, and so on.
The codes are then turned into actual types via an operator $\El$, so if $C : \Univ$, then $\El(C)$ is a type.
Further, we add judgemental equalities which postulate that the $\El$ operator works as expected: $\El(A \bar{×} B) = \El(A) × \El(B)$ and so on.

Type theories with only one universe are also commonly discussed.
This restriction makes the metatheory simpler, and the generalisation to infinitely many universes is often considered mere busywork.
However, in this case, the universe cannot be a member of itself, so we cannot have $\Univ : \Univ$, as the type theory would then become inconsistent per Girard's paradox~\cite{girard:thesis}.
As a result, the universe cannot itself have a type.
This forces us to introduce two additional judgements: $Γ ⊢ A~\type$, indicating that $A$ is a well-formed type, and $Γ ⊢ A ≡ B~\type$, indicating that $A$ and $B$ are judgementally equal types.

As another variation, we can make our universes \emph{cumulative}.
This means that any type $T$ in the $n$-th universe $\Univ_{n}$ is automatically also included in all higher universes $\Univ_{n+1}, \Univ_{n+2}, \dots$.
The type theory of Coq~\cite{coq} includes a corresponding rule.%
\footnote{The most precise description of the type theory actually implemented in Coq appears in Coq's handbook.
  The current version of the handbook is included in the cited artifact.
  The typing rules (excluding those for inductive types) are given on pp.\ 22--25 and conversion is discussed on p.\ 26.}
However, cumulativity seems to be only of minor importance when working with a type theory in practice, so most systems eschew it to obtain a simpler theory.

\bibliography{lit}

\end{document}
